{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please cite:\n",
      "Bizzego et al. (2019) 'pyphysio: A physiological signal processing library for data science approaches in physiology', SoftwareX\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from scipy import stats\n",
    "import pyphysio as ph\n",
    "from pdb import set_trace\n",
    "import random\n",
    "from datetime import timedelta, datetime\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-6d888cb823e8>:3: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  surv = pd.read_csv('/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/10 Minute Windows/PR003/PR003 Survey Windows MERGED.csv',parse_dates=['timestamp'], infer_datetime_format=True, index_col=[0])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/10 Minute Windows/PR003/PR003 Survey Windows MERGED.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6d888cb823e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#survey windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#these are available in the 10 Minute Windows subfolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msurv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/10 Minute Windows/PR003/PR003 Survey Windows MERGED.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#baseline windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1661\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/10 Minute Windows/PR003/PR003 Survey Windows MERGED.csv'"
     ]
    }
   ],
   "source": [
    "#survey windows \n",
    "#these are available in the 10 Minute Windows subfolder\n",
    "surv = pd.read_csv('/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/10 Minute Windows/PR003/PR003 Survey Windows MERGED.csv',parse_dates=['timestamp'], infer_datetime_format=True, index_col=[0])\n",
    "\n",
    "#baseline windows\n",
    "#also available in the 10 Minute Windows subfolder\n",
    "base = pd.read_csv('/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/10 Minute Windows/PR003/PR003 Baseline MERGED.csv',parse_dates=['timestamp'], infer_datetime_format=True, index_col=[0])\n",
    "\n",
    "#ema responses\n",
    "#found in Sofie Data subfolder, may have to dig for it \n",
    "ema = pd.read_csv(\"/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/Data/Sofie Data/PR/Maintained active dx/PR003/PR003_rawwithtime2022-07-03.csv\", parse_dates=['ethica_time_utc'], infer_datetime_format=True, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### these scripts are taken from prior scripts written by collaborators with some modification by me\n",
    "### these are all the functions we need... i think\n",
    "\n",
    "#Preprocessing filters that will be applied to raw data samples\n",
    "def exp_moving_average(signal, w):\n",
    "    \"\"\"Expoential moving average filter from pandas\"\"\"\n",
    "    return pd.Series(signal.ewm(span=w, adjust=True).mean(), signal.index)\n",
    "\n",
    "def filt_EDA(df_data):\n",
    "    \"\"\"Apply filter to EDA signal using processing steps from Pyphysio.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_data : pandas.DataFrame\n",
    "        DataFrame with EDA time and signal data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame \n",
    "        Updated DataFrame with Tonic and Phasic signals\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up Pyphysio EvenlySignal object\n",
    "    eda_data = ph.EvenlySignal(values = df_data['EDA'].to_numpy(), sampling_freq = 4, signal_type='EDA')\n",
    "    \n",
    "    # Apply IIR filter\n",
    "    eda_data = ph.IIRFilter(fp=0.8, fs=1.1, ftype='ellip')(eda_data)\n",
    "    driver = ph.DriverEstim()(eda_data)\n",
    "    \n",
    "    # Estimate Tonic and Phasic signals\n",
    "    phasic, tonic, _ = ph.PhasicEstim(delta=0.02)(driver)\n",
    "    \n",
    "    # Adjust signal length to match length of original EDA signal\n",
    "    if len(phasic) != len(eda_data.get_values()):\n",
    "        phasic = np.append(phasic.get_values(), phasic[-1])\n",
    "        tonic = np.append(tonic.get_values(), tonic[-1])    \n",
    "    \n",
    "    # Append signal data to DataFrame\n",
    "    df_data.loc[:,'Tonic'] = tonic\n",
    "    df_data.loc[:,'Phasic'] = phasic\n",
    "    return df_data  \n",
    "\n",
    "def filt_TEMP(df_data):\n",
    "    \"\"\"Apply filter to TEMP signal using predetermined values\"\"\"\n",
    "    df_data['TEMP_Filtered'] = exp_moving_average(df_data['TEMP'],60)\n",
    "    return df_data\n",
    "\n",
    "def filter_signals(df_data):\n",
    "    \"\"\"Apply filters/processing to respective signals.\n",
    "    \n",
    "    This function is intended to be used in conjunction with the\n",
    "    pandas.DataFrame.apply() method, which passes a column of a DataFrame at\n",
    "    a time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_data : pandas.Series\n",
    "        A Series of DataFrames which contain all the signals for a single\n",
    "        session.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Updated Series of DataFrames containing filtered/processed signal\n",
    "        data\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    No filtering/processing for ACC, HR, and IBI signals is implemented,\n",
    "    but is commented out for implementation in the future.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #df_data.loc['BVP'].loc[:,'BVP'] = filt_BVP(df_data.loc['BVP'])\n",
    "    df_data = filt_EDA(df_data)\n",
    "    df_data = filt_TEMP(df_data)\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "#RMS calculation Helper Function\n",
    "def rms(data):\n",
    "    return np.sqrt(np.mean(data ** 2))\n",
    "\n",
    "#Every other feature is easy to calculate using Python built-ins\n",
    "def feature_extract(df_data):\n",
    "    result = {}\n",
    "    result['Time'] = df_data.loc[:,'timestamp'].min()\n",
    "    #for featbase in ['HR','EDA','TEMP','Tonic','Phasic','TEMP_Filtered']:\n",
    "    for featbase in ['HR','EDA','TEMP', 'meanCenteredEDA', 'meanCenteredHR', 'meanCenteredTEMP']:\n",
    "        #set_trace()\n",
    "        result[featbase + '_Mean'] = df_data.loc[:,featbase].mean()\n",
    "        result[featbase + '_Minimum'] = df_data.loc[:,featbase].min()\n",
    "        result[featbase + '_Maximum'] = df_data.loc[:,featbase].max()\n",
    "        result[featbase + '_Stdev'] = df_data.loc[:,featbase].std()\n",
    "        result[featbase + '_RMS'] = rms(df_data.loc[:,featbase])\n",
    "        result[featbase + '_MAD'] = df_data.loc[:,featbase].mad()\n",
    "        result[featbase + '_MAV'] = df_data.loc[:,featbase].abs().max()\n",
    "        result[featbase + '_Median'] = df_data.loc[:,featbase].median()\n",
    "        result[featbase + '_P25'] = df_data.loc[:,featbase].quantile(0.25)\n",
    "        result[featbase + '_P75'] = df_data.loc[:,featbase].quantile(0.75)\n",
    "    return pd.Series(result, dtype='object')\n",
    "\n",
    "\n",
    "#simplified the feature extraction function to only grab mean and median\n",
    "def average_calc(df_data):\n",
    "    result = {}    \n",
    "    result['EDA_Mean'] = df_data.loc[:,'EDA'].mean()\n",
    "    result['EDA_Median'] = df_data.loc[:,'EDA'].median()\n",
    "    result['HR_Mean'] = df_data.loc[:,'HR'].mean()\n",
    "    result['HR_Median'] = df_data.loc[:,'HR'].median()\n",
    "    result['TEMP_Mean'] = df_data.loc[:,'TEMP'].mean()\n",
    "    result['TEMP_Median'] = df_data.loc[:,'TEMP'].median()\n",
    "\n",
    "    return pd.Series(result, dtype='object')\n",
    "\n",
    "#creating a function to match rows\n",
    "#remember that the physio timestamp must ALWAYS BE BEFORE the EMA timestamp\n",
    "\n",
    "def windowMatch(features,surveys): \n",
    "    matchlist = []\n",
    "    for index, frow in features.iterrows():\n",
    "        physiotime = frow['Time']\n",
    "        for index, srow in surveys.iterrows():\n",
    "            ematime = srow['ethica_time_utc']\n",
    "            df3 = ematime - physiotime\n",
    "            if df3 < delta and df3 > zeropoint:\n",
    "                #print(\"Physio time (\", physiotime, \") and EMA time (\", ematime, \") difference is\", df3)\n",
    "                matchlist.append(pd.concat([frow,srow],axis=0))\n",
    "    return pd.DataFrame(matchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this chunk of code will normalize our date (deducting the average/median of the baseline windows from each datapoint in survey windows)\n",
    "\n",
    "baseline_values = base.groupby(['code']).apply(average_calc) # calculating averages of the baseline\n",
    "\n",
    "#creating variables to subtract from the dataframe columns\n",
    "edamean = baseline_values.at[0,\"EDA_Mean\"]\n",
    "edamed = baseline_values.at[0,\"EDA_Median\"]\n",
    "hrmean = baseline_values.at[0,\"HR_Mean\"]\n",
    "hrmed = baseline_values.at[0,\"HR_Median\"]\n",
    "tempmean = baseline_values.at[0,\"TEMP_Mean\"]\n",
    "tempmed = baseline_values.at[0,\"TEMP_Median\"]\n",
    "\n",
    "#subtracting our means/medians from each value in the participant's data - excluding ACC \n",
    "surv['meanCenteredEDA'] = surv['EDA'] - edamean\n",
    "surv['medianCenteredEDA'] = surv['EDA'] - edamed\n",
    "surv['meanCenteredHR'] = surv['HR'] - hrmean\n",
    "surv['medianCenteredHR'] = surv['HR'] - hrmed\n",
    "surv['meanCenteredTEMP'] = surv['TEMP'] - tempmean\n",
    "surv['medianCenteredTEMP'] = surv['TEMP'] - tempmed\n",
    "\n",
    "surv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this chunk is to prepare the physio data to be matched to timestamps \n",
    "\n",
    "physio = surv.sort_values(['timestamp'], ignore_index=True) #sort by timestamp\n",
    "physio = physio.groupby(['event']).apply(feature_extract) #applying feature extraction to create summary features\n",
    "\n",
    "#setting up a timedelta to base our window matching off of\n",
    "#a survey window can never occur more than an hour from when a survey was completed, so our delta is 1 hour\n",
    "#if a window is more than an hour separate from survey completion, it cannot be matched\n",
    "#however, the survey window timestamp should ALWAYS precede the survey completion time\n",
    "delta = timedelta(hours = 1)\n",
    "zeropoint = timedelta(seconds = 1)\n",
    "\n",
    "#now we apply the window matching function - it calculates the time differences of each timestamp\n",
    "#if the time difference is less than 1 hour, match the windows together\n",
    "mydf = windowMatch(physio, ema)\n",
    "for index,row in mydf.iterrows():\n",
    "    print (abs(row['ethica_time_utc']-row['Time']))\n",
    "    \n",
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting together physio featureset to be ready for correlations\n",
    "featbase =['HR','EDA','TEMP', 'meanCenteredEDA', 'meanCenteredHR', 'meanCenteredTEMP']\n",
    "featstat =['_Mean','_Minimum','_Stdev','_RMS','_MAD','_MAV','_Median','_P25','_P75']\n",
    "physio_feats = {}\n",
    "allfeats = []\n",
    "for fb in featbase:\n",
    "    curfeats=[]\n",
    "    for fs in featstat:\n",
    "        curfeats.append(fb+fs)\n",
    "        allfeats.append(fb+fs)\n",
    "    physio_feats[fb] = mydf[curfeats]\n",
    "physio_feats['All'] = mydf[allfeats]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centering EMA just in case - just hashtag out this loop if it's not necessary\n",
    "\n",
    "for col in ema_feats:\n",
    "    ema_feats[col + '_meanCentered'] = ema_feats[col] - ema_feats.loc[:,col].mean()\n",
    "    ema_feats[col + '_medCentered'] = ema_feats[col] - ema_feats.loc[:,col].median()\n",
    "    \n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "ema_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cleaning up EMA dataset for correlations\n",
    "excl_list = ['ethica_time','lag','tdif','cumsumT','ethica_time_utc','dayvar','beepvar','beepconsec']\n",
    "surfeat=[ele for ele in list(ema.columns) if ele not in excl_list]\n",
    "ema_feats = mydf[surfeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we're correlating physio with EMA here\n",
    "corrdict = {}\n",
    "for sur in surfeat:\n",
    "    corrdict[sur]=physio_feats['All'].corrwith(ema_feats[sur],method='pearson')\n",
    "    corrdict[sur]=physio_feats['All'].corrwith(ema_feats[sur],method='pearson')\n",
    "corr_df = pd.DataFrame(corrdict)\n",
    "                                                     \n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#just in case you want to do some scatterplots \n",
    "#you'll notice a lot of outliers in some of these plots - we may want to establish some way to remove outliers - perhaps if something is 4+ SDs from the mean?\n",
    "ax1= mydf.plot.scatter(x='HR_Mean',y='restrict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/JPCS Special Issue/normalizing.ipynb\n",
    "\n",
    "#this will save the centered survey windows\n",
    "surv.to_csv(\"\"\"/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/JPCS Special Issue/Normalized Data/PR003 Survey Windows CENTERED.csv\"\"\")\n",
    "\n",
    "#this will save our correlation matrix \n",
    "corr_df.to_csv(\"\"\"/Users/sandoval/Library/CloudStorage/Box-Box/R15 Sensor Preprocessing and Analysis/JPCS Special Issue/Correlation Matrices/PR003 Survey Correlations.csv\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
